{"hash":"fa5fedf1848133ded319caa58f5dc027d2a72899","data":{"markdownPage":{"id":"59a15b326dd00387c07c16407776b927","title":"CDC(Change Data Capture) with Kafka","description":"","path":"/development/kafka-connect/","timeToRead":6,"content":"<h1 id=\"cdcchange-data-capture-with-kafka\"><a href=\"#cdcchange-data-capture-with-kafka\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>CDC(Change Data Capture) with Kafka</h1>\n<h3 id=\"kafka-connect에-의한-데이터-동기화\"><a href=\"#kafka-connect%EC%97%90-%EC%9D%98%ED%95%9C-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%8F%99%EA%B8%B0%ED%99%94\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Kafka Connect에 의한 데이터 동기화</h3>\n<ul>\n<li>Kafka Connect를 이용한 CDC(Change Data Capture)를 통해 주문팀에서 생성된 데이터가 추천상품을 위해, 패턴 분석이 필요한 마케팅팀에 동기화 되는지를 실습한다. </li>\n<li>Connect는 Connector를 실행시켜주는 서버로 DB동기화시, 벤더사가 만든 Connector, 또는 OSS(Debezium, Confluent) 계열의 Connector를 사용한다. </li>\n<li>Lab에서는 MySQL DB를 설치하여 CDC 테스트를 적용한다. MySQL DB는 시작과 동시에 실행된다. (3306 Open)</li>\n</ul>\n<h3 id=\"kafka-서버-실행\"><a href=\"#kafka-%EC%84%9C%EB%B2%84-%EC%8B%A4%ED%96%89\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Kafka 서버 실행</h3>\n<h4 id=\"connector-다운로드\"><a href=\"#connector-%EB%8B%A4%EC%9A%B4%EB%A1%9C%EB%93%9C\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Connector 다운로드</h4>\n<ul>\n<li>Kafka Connect를 위한 JDBC 드라이브를 다운로드한다.</li>\n</ul>\n<pre class=\"language-sh\"><code class=\"language-sh\">git clone https://github.com/acmexii/kafka-connect.git\ncd kafka-connect</code></pre>\n<h4 id=\"kafka-수동-실행\"><a href=\"#kafka-%EC%88%98%EB%8F%99-%EC%8B%A4%ED%96%89\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Kafka 수동 실행</h4>\n<p>kafka를 수동설치 후, zookeeper를 실행한다.</p>\n<pre class=\"language-sh\"><code class=\"language-sh\">curl \"https://archive.apache.org/dist/kafka/2.7.1/kafka_2.13-2.7.1.tgz\" -o ./kafka-2.7.1.tgz\ntar xvfz kafka-2.7.1.tgz</code></pre>\n<pre class=\"language-text\"><code class=\"language-text\">cd kafka_2.13-2.7.1/\nbin/zookeeper-server-start.sh config/zookeeper.properties &amp;</code></pre>\n<ul>\n<li>2181 포트로 zookeeper가 실행된다.</li>\n<li>새로운 터미널에서 kafka 데몬을 실행한다.</li>\n</ul>\n<pre class=\"language-sh\"><code class=\"language-sh\">cd kafka-connect\ncd kafka_2.13-2.7.1/\nbin/kafka-server-start.sh config/server.properties &</code></pre>\n<h3 id=\"kafka-connect-서버-실행\"><a href=\"#kafka-connect-%EC%84%9C%EB%B2%84-%EC%8B%A4%ED%96%89\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Kafka Connect 서버 실행</h3>\n<h4 id=\"jdbc-connector-설치\"><a href=\"#jdbc-connector-%EC%84%A4%EC%B9%98\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>JDBC Connector 설치</h4>\n<ul>\n<li>Connect 서버 실행전, 동기화 대상 데이터베이스의 JDBC 드라이버를 설치한다.</li>\n<li>Connector를 설치할 폴더를 생성한다.</li>\n</ul>\n<pre class=\"language-sh\"><code class=\"language-sh\">cd kafka-connect/kafka_2.13-2.7.1/\nexport kafka_home=$PWD\necho \"export kafka_home=/workspace/kafka-cdc/kafka-connect/kafka_2.13-2.7.1/\" >> ~/.bashrc\nmkdir connectors\ncd connectors</code></pre>\n<ul>\n<li>다운받은 confluentinc-kafka-connect-jdbc-10.2.5.zip을 복사 후 unzip 한다. </li>\n</ul>\n<pre class=\"language-sh\"><code class=\"language-sh\">cp ../../confluentinc-kafka-connect-jdbc-10.2.5.zip ./\nunzip confluentinc-kafka-connect-jdbc-10.2.5.zip</code></pre>\n<ul>\n<li>$kafka_home/config 폴더로 이동 후 connect-distributed.properties에 unzip한 폴더를 등록해 준다.</li>\n</ul>\n<pre class=\"language-sh\"><code class=\"language-sh\">cd $kafka_home/config \nvi connect-distributed.properties</code></pre>\n<ul>\n<li>마지막 행으로 이동하여 주석을 제거한다.</li>\n</ul>\n<pre class=\"language-text\"><code class=\"language-text\">plugin.path=/workspace/kafka-cdc/kafka-connect/kafka_2.13-2.7.1/connectors</code></pre>\n<ul>\n<li>위와 같이 편집하고 저장종료한다. </li>\n</ul>\n<h4 id=\"kafka-connect-서버-실행-1\"><a href=\"#kafka-connect-%EC%84%9C%EB%B2%84-%EC%8B%A4%ED%96%89-1\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Kafka Connect 서버 실행</h4>\n<ul>\n<li>Connect CDC Server를 실행한다. </li>\n</ul>\n<pre class=\"language-sh\"><code class=\"language-sh\">cd $kafka_home\nbin/connect-distributed.sh config/connect-distributed.properties </code></pre>\n<ul>\n<li>Kafka Connect는 default 8083 포트로 실행이 된다. </li>\n</ul>\n<ul>\n<li>Connect 서버 실행 후, Kafka Server의 Topic을 확인해 본다.</li>\n</ul>\n<pre class=\"language-sh\"><code class=\"language-sh\">$kafka_home/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list</code></pre>\n<ul>\n<li>\n<p>Connect를 위한 토픽이 추가되었다.</p>\n<blockquote>\n<p>connect-configs, connect-offsets, connect-status</p>\n</blockquote>\n</li>\n</ul>\n<h3 id=\"connector-설치\"><a href=\"#connector-%EC%84%A4%EC%B9%98\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Connector 설치</h3>\n<h4 id=\"source-connector-설치\"><a href=\"#source-connector-%EC%84%A4%EC%B9%98\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Source Connector 설치</h4>\n<ul>\n<li>Kafka connect의 REST API를 통해 Source 및 Sink connector를 등록한다. </li>\n</ul>\n<pre class=\"language-curl\"><code class=\"language-curl\">curl -i -X POST -H \"Accept:application/json\" \\\n    -H  \"Content-Type:application/json\" http://localhost:8083/connectors/ \\\n    -d '{\n    \"name\": \"mysql-source-connector\",\n    \"config\": {\n        \"connector.class\": \"io.confluent.connect.jdbc.JdbcSourceConnector\",\n        \"connection.url\": \"jdbc:mysql://localhost:3306/my-database?useSSL=false&allowPublicKeyRetrieval=true&serverTimezone=UTC&characterEncoding=UTF-8\",\n        \"connection.user\":\"root\",\n        \"connection.password\":\"1234\",\n        \"mode\":\"incrementing\",\n        \"useSSL\":\"false\",\n        \"incrementing.column.name\" : \"id\",\n        \"table.whitelist\" : \"ORDER_TABLE\",\n        \"topic.prefix\" : \"SYNC_\",\n        \"tasks.max\" : \"1\"\n    }\n}'</code></pre>\n<blockquote>\n<p>Connector 등록시, 'No suitable driver' 오류가 발생할 경우, Classpath에 mysql driver를 설정해 준다.</p>\n</blockquote>\n<ul>\n<li>등록한 Connector를 확인한다.</li>\n</ul>\n<pre class=\"language-sh\"><code class=\"language-sh\">http localhost:8083/connectors</code></pre>\n<h4 id=\"order-마이크로서비스-설정\"><a href=\"#order-%EB%A7%88%EC%9D%B4%ED%81%AC%EB%A1%9C%EC%84%9C%EB%B9%84%EC%8A%A4-%EC%84%A4%EC%A0%95\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Order 마이크로서비스 설정</h4>\n<ul>\n<li>주문 서비스가 로컬에 실행된 MySQL DB를 사용한다.</li>\n<li>Order의 application.yml을 열어 default profile의 datasource를 확인한다.</li>\n</ul>\n<pre class=\"language-yaml\"><code class=\"language-yaml\">  <span class=\"token key atrule\">datasource</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">url</span><span class=\"token punctuation\">:</span> jdbc<span class=\"token punctuation\">:</span>mysql<span class=\"token punctuation\">:</span>//$<span class=\"token punctuation\">{</span>_DATASOURCE_ADDRESS<span class=\"token punctuation\">:</span>localhost<span class=\"token punctuation\">:</span><span class=\"token number\">3306</span><span class=\"token punctuation\">}</span>/$<span class=\"token punctuation\">{</span>_DATASOURCE_TABLESPACE<span class=\"token punctuation\">:</span>my<span class=\"token punctuation\">-</span>database<span class=\"token punctuation\">}</span><span class=\"token punctuation\">?</span>serverTimezone=UTC<span class=\"token important\">&amp;characterEncoding</span>=UTF<span class=\"token punctuation\">-</span><span class=\"token number\">8</span>\n    <span class=\"token key atrule\">username</span><span class=\"token punctuation\">:</span> $<span class=\"token punctuation\">{</span>_DATASOURCE_USERNAME<span class=\"token punctuation\">:</span>root<span class=\"token punctuation\">}</span>\n    <span class=\"token key atrule\">password</span><span class=\"token punctuation\">:</span> $<span class=\"token punctuation\">{</span>_DATASOURCE_PASSWORD<span class=\"token punctuation\">:</span><span class=\"token number\">1234</span><span class=\"token punctuation\">}</span>\n    <span class=\"token key atrule\">driverClassName</span><span class=\"token punctuation\">:</span> com.mysql.cj.jdbc.Driver </code></pre>\n<h4 id=\"소스-테이블에-data-입력\"><a href=\"#%EC%86%8C%EC%8A%A4-%ED%85%8C%EC%9D%B4%EB%B8%94%EC%97%90-data-%EC%9E%85%EB%A0%A5\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>소스 테이블에 Data 입력</h4>\n<ul>\n<li>order 마이크로서비스를 기동하고 소스 테이블에 샘플데이터를 생성(주문발행)한다.</li>\n</ul>\n<pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token builtin class-name\">cd</span> order\nmvn spring-boot:run</code></pre>\n<pre class=\"language-text\"><code class=\"language-text\">http POST :8081/orders productId=1 qty=10 customerId=1000 price=10000\nhttp POST :8081/orders productId=2 qty=20 customerId=2000 price=20000</code></pre>\n<ul>\n<li>Kafka topic을 확인해 본다.</li>\n</ul>\n<pre class=\"language-sh\"><code class=\"language-sh\">$kafka_home/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list</code></pre>\n<ul>\n<li>\n<p>'SYNC_ORDER_TABLE' 토픽이 추가되어 목록에 나타난다.</p>\n<blockquote>\n<p>Kafka Connect는 테이블 단위로 토픽이 생성되어 Provider와 Consumer간 데이터를 Sync합니다. </p>\n</blockquote>\n</li>\n</ul>\n<pre class=\"language-text\"><code class=\"language-text\">$kafka_home/bin/kafka-console-consumer.sh --bootstrap-server 127.0.0.1:9092 --topic SYNC_ORDER_TABLE --from-beginning</code></pre>\n<h4 id=\"sink-connector-설치\"><a href=\"#sink-connector-%EC%84%A4%EC%B9%98\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Sink Connector 설치</h4>\n<pre class=\"language-curl\"><code class=\"language-curl\">curl -i -X POST -H \"Accept:application/json\" \\\n    -H  \"Content-Type:application/json\" http://localhost:8083/connectors/ \\\n    -d '{\n    \"name\": \"mysql-sink-connector\",\n    \"config\": {\n        \"connector.class\": \"io.confluent.connect.jdbc.JdbcSinkConnector\",\n        \"connection.url\": \"jdbc:mysql://localhost:3306/my-database?useSSL=false&allowPublicKeyRetrieval=true&serverTimezone=UTC&characterEncoding=UTF-8\",\n        \"connection.user\":\"root\",\n        \"connection.password\":\"1234\",\n        \"useSSL\":\"false\",        \n        \"auto.create\":\"true\",       \n        \"auto.evolve\":\"true\",       \n        \"delete.enabled\":\"false\",\n        \"tasks.max\":\"1\",\n        \"topics\":\"SYNC_ORDER_TABLE\"\n    }\n}'</code></pre>\n<h4 id=\"mysql-client로-복제된-테이블-확인하기\"><a href=\"#mysql-client%EB%A1%9C-%EB%B3%B5%EC%A0%9C%EB%90%9C-%ED%85%8C%EC%9D%B4%EB%B8%94-%ED%99%95%EC%9D%B8%ED%95%98%EA%B8%B0\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>mysql client로 복제된 테이블 확인하기</h4>\n<ul>\n<li>새 터미널을 열고 아래 커맨드로 Mysql Client를 실행한다.</li>\n</ul>\n<pre class=\"language-text\"><code class=\"language-text\">cd mysql\ndocker-compose exec -it master-server bash\nmysql --user=root --password=1234\nuse my-database;\nshow tables;</code></pre>\n<ul>\n<li>Kafka Connect 서버가 복제한 SYNC_ORDER_TABLE 이 존재한다.</li>\n</ul>\n<h4 id=\"sync_order_table을-사용하는-marketing-서비스-실행\"><a href=\"#sync_order_table%EC%9D%84-%EC%82%AC%EC%9A%A9%ED%95%98%EB%8A%94-marketing-%EC%84%9C%EB%B9%84%EC%8A%A4-%EC%8B%A4%ED%96%89\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>SYNC_ORDER_TABLE을 사용하는 Marketing 서비스 실행</h4>\n<ul>\n<li>marketing 서비스가 로컬에 실행된 MySQL DB를 사용한다.</li>\n<li>marketing 서비스의 application.yml을 열어 default profile의 datasource를 확인한다.</li>\n</ul>\n<pre class=\"language-yaml\"><code class=\"language-yaml\">  <span class=\"token key atrule\">datasource</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">url</span><span class=\"token punctuation\">:</span> jdbc<span class=\"token punctuation\">:</span>mysql<span class=\"token punctuation\">:</span>//$<span class=\"token punctuation\">{</span>_DATASOURCE_ADDRESS<span class=\"token punctuation\">:</span>localhost<span class=\"token punctuation\">:</span><span class=\"token number\">3306</span><span class=\"token punctuation\">}</span>/$<span class=\"token punctuation\">{</span>_DATASOURCE_TABLESPACE<span class=\"token punctuation\">:</span>my<span class=\"token punctuation\">-</span>database<span class=\"token punctuation\">}</span><span class=\"token punctuation\">?</span>serverTimezone=UTC<span class=\"token important\">&amp;characterEncoding</span>=UTF<span class=\"token punctuation\">-</span><span class=\"token number\">8</span>\n    <span class=\"token key atrule\">username</span><span class=\"token punctuation\">:</span> $<span class=\"token punctuation\">{</span>_DATASOURCE_USERNAME<span class=\"token punctuation\">:</span>root<span class=\"token punctuation\">}</span>\n    <span class=\"token key atrule\">password</span><span class=\"token punctuation\">:</span> $<span class=\"token punctuation\">{</span>_DATASOURCE_PASSWORD<span class=\"token punctuation\">:</span><span class=\"token number\">1234</span><span class=\"token punctuation\">}</span>\n    <span class=\"token key atrule\">driverClassName</span><span class=\"token punctuation\">:</span> com.mysql.cj.jdbc.Driver </code></pre>\n<ul>\n<li>새 터미널에서 marketing 서비스를 실행하고, 분석을 위해 동기화된 데이터를 조회한다.</li>\n</ul>\n<pre class=\"language-text\"><code class=\"language-text\">http GET :8082/syncOrders</code></pre>\n<ul>\n<li>Sink Connector를 통해 주문서비스에서 입력한 데이터가 CDC를 통해 마케팅 테이블(SYNC_ORDER_TABLE)에 복제된 데이터가 조회된다.</li>\n</ul>\n<ul>\n<li>다시한번 Orders 테이블에 데이터를 입력하고 마케팅팀에 주문 데이터 동기화가 되는지 확인해 본다.</li>\n</ul>\n<pre class=\"language-bash\"><code class=\"language-bash\">http POST :8081/orders <span class=\"token assign-left variable\">productId</span><span class=\"token operator\">=</span><span class=\"token number\">1</span> <span class=\"token assign-left variable\">qty</span><span class=\"token operator\">=</span><span class=\"token number\">10</span> <span class=\"token assign-left variable\">customerId</span><span class=\"token operator\">=</span><span class=\"token number\">1000</span> <span class=\"token assign-left variable\">price</span><span class=\"token operator\">=</span><span class=\"token number\">10000</span>\nhttp GET :8082/syncOrders</code></pre>\n<h3 id=\"source-connector-mode\"><a href=\"#source-connector-mode\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Source Connector Mode</h3>\n<ul>\n<li>Lab에서 사용한 jdbc Source Connector의 incrementing 모드에서는 기존 row의 수정이나 삭제는 감지하지 못한다. </li>\n<li>Mode 옵션</li>\n</ul>\n<pre class=\"language-text\"><code class=\"language-text\">1) bulk : 데이터를 폴링할 때 마다 전체 테이블을 복사\n\n2) incrementing : 특정 컬럼의 중가분만 감지되며, 기존 행의 수정과 삭제는 감지되지 않음\nincrementing.column.name : incrementing 모드에서 새 행을 감지하는데 사용할 컬럼명\n\n3) timestamp : timestamp형 컬럼일 경우, 새 행과 수정된 행을 감지함\ntimestamp.column.name : timestamp 모드에서 대상 행을 감지하는데 사용할 컬럼명\n\n4) timestamp+incrementing : 위의 두 컬럼을 모두 사용하는 옵션</code></pre>\n<p>참고 URL: <a href=\"https://presentlee.tistory.com/7\" target=\"_blank\" rel=\"noopener noreferrer\">https://presentlee.tistory.com/7</a></p>\n","sidebar":"started","next":"","prev":"","headings":[{"depth":1,"value":"CDC(Change Data Capture) with Kafka","anchor":"#cdcchange-data-capture-with-kafka"},{"depth":3,"value":"Kafka Connect에 의한 데이터 동기화","anchor":"#kafka-connect에-의한-데이터-동기화"},{"depth":3,"value":"Kafka 서버 실행","anchor":"#kafka-서버-실행"},{"depth":4,"value":"Connector 다운로드","anchor":"#connector-다운로드"},{"depth":4,"value":"Kafka 수동 실행","anchor":"#kafka-수동-실행"},{"depth":3,"value":"Kafka Connect 서버 실행","anchor":"#kafka-connect-서버-실행"},{"depth":4,"value":"JDBC Connector 설치","anchor":"#jdbc-connector-설치"},{"depth":4,"value":"Kafka Connect 서버 실행","anchor":"#kafka-connect-서버-실행-1"},{"depth":3,"value":"Connector 설치","anchor":"#connector-설치"},{"depth":4,"value":"Source Connector 설치","anchor":"#source-connector-설치"},{"depth":4,"value":"Order 마이크로서비스 설정","anchor":"#order-마이크로서비스-설정"},{"depth":4,"value":"소스 테이블에 Data 입력","anchor":"#소스-테이블에-data-입력"},{"depth":4,"value":"Sink Connector 설치","anchor":"#sink-connector-설치"},{"depth":4,"value":"mysql client로 복제된 테이블 확인하기","anchor":"#mysql-client로-복제된-테이블-확인하기"},{"depth":4,"value":"SYNC_ORDER_TABLE을 사용하는 Marketing 서비스 실행","anchor":"#sync_order_table을-사용하는-marketing-서비스-실행"},{"depth":3,"value":"Source Connector Mode","anchor":"#source-connector-mode"}]},"allMarkdownPage":{"edges":[{"node":{"path":"/tool/marketplace/","title":"Template/Topping Marketplace"}},{"node":{"path":"/tool/infrastructure-modeling/","title":"Infrastructure Modeling (Kubernetes)"}},{"node":{"path":"/operations/istio-metric-based-hpa/","title":"[Service Mesh] Istio Metrics based HPA"}},{"node":{"path":"/development/ops-docker/","title":"Application Packaging with Container (Docker)"}},{"node":{"path":"/tool/google-drive-examples/","title":"Google Drive Examples"}},{"node":{"path":"/tool/on-prem-inst/","title":"Installing on-premise MSA-Easy"}},{"node":{"path":"/tool/event-storming-tool/","title":"EventStorming"}},{"node":{"path":"/tool/cloud-ide-tool/","title":"Cloud IDE"}},{"node":{"path":"/tool/development-practice/","title":"Registration Course"}},{"node":{"path":"/tool/attending-lectures/","title":"Attending lectures"}},{"node":{"path":"/operations/ops-persistence-volume-azure/","title":"파일공유를 위한 클라우드 스토리지 활용(Azure)"}},{"node":{"path":"/templates-language/python-template/","title":"Python template "}},{"node":{"path":"/templates-language/springboot-java-template/","title":"Spring Boot/Java Template"}},{"node":{"path":"/tool/chat-gpt/","title":"Chat GPT-based Model Generation"}},{"node":{"path":"/templates-language/go-template/","title":"Go Template "}},{"node":{"path":"/operations/service/","title":"12번가 마이크로서비스 라우터(Service) 생성"}},{"node":{"path":"/operations/ops-utility/","title":"쿠버네티스 유틸리티"}},{"node":{"path":"/operations/service-mesh-ab-testing/","title":"Istio based A/B testing 배포"}},{"node":{"path":"/operations/ops-service-mesh-istio-2/","title":"[Service Mesh] Istio-2"}},{"node":{"path":"/operations/ops-persistence-volume/","title":"파일시스템 (볼륨) 연결과 데이터베이스 설정"}},{"node":{"path":"/operations/ops-service-mesh-istio/","title":"[Service Mesh] Istio"}},{"node":{"path":"/operations/ops-readiness/","title":"셀프힐링 & 무정지 배포 실습"}},{"node":{"path":"/operations/ops-pod-status/","title":"Pod 상태값에 따른 마이크로서비스 트러블 슈팅"}},{"node":{"path":"/operations/ops-persistence-volume-efs/","title":"파일공유를 위한 NAS 스토리지 생성과 설정"}},{"node":{"path":"/operations/ops-persistence-volume-gcp/","title":"파일공유를 위한 클라우드 스토리지 활용(GCP)"}},{"node":{"path":"/operations/ops-liveness/","title":"셀프힐링 실습"}},{"node":{"path":"/operations/ops-ingress-virtualhost/","title":"Ingress - Virtual Host based"}},{"node":{"path":"/operations/ops-label-annotation/","title":"Labels and Annotations"}},{"node":{"path":"/operations/ops-kubernetes/","title":"Kubernetes Basic Command"}},{"node":{"path":"/operations/ops-ingress/","title":"Ingress 를 통한 진입점 통일 - Path-based routing"}},{"node":{"path":"/operations/ops-aws-setting/","title":"AWS Cloud Setup(EKS, ECR 설정)"}},{"node":{"path":"/operations/ops-configmap/","title":"Kubernetes에 환경변수 구성하기"}},{"node":{"path":"/operations/ops-deploy-my-app/","title":"애플리케이션 패키징,도커라이징,클러스터 배포"}},{"node":{"path":"/operations/ops-aws-csi-setting/","title":"AWS Cloud Setup(Container Storage Interface)"}},{"node":{"path":"/operations/ops-autoscale/","title":"Pod Auto Scaling"}},{"node":{"path":"/operations/ops-argo-rollout-canary-istio/","title":"[GitOps] Argo Rollout 와 Istio 를 통한 카나리 배포"}},{"node":{"path":"/operations/ops-anatomy-kubernetes/","title":"쿠버네티스 내부구조 분석"}},{"node":{"path":"/operations/microservice-logging2/","title":"마이크로서비스 통합 로깅 with Loki stack"}},{"node":{"path":"/operations/istio-sre-monitoring/","title":"Service Reliability Engineering"}},{"node":{"path":"/operations/microservice-logging/","title":"마이크로서비스 통합 로깅 with EFK stack"}},{"node":{"path":"/operations/istio-traffic/","title":"[Service Mesh] Istio 를 통한 동적 트래픽 라우팅"}},{"node":{"path":"/operations/k8s-monitoring/","title":"MSA 모니터링 with installing Grafana"}},{"node":{"path":"/operations/istio-resiliency-part2/","title":"[Service Mesh] Istio 를 통한 서비스 회복성 Part2 - 서킷브레이커"}},{"node":{"path":"/operations/istio-msa-telemetry/","title":"[Service Mesh] MSA 모니터링 w/ Istio addon Grafana"}},{"node":{"path":"/operations/istio-resiliency-part1/","title":"[Service Mesh] Istio 를 통한 서비스 회복성 Part1 - 타임아웃/재시도"}},{"node":{"path":"/started/domain-driven/","title":"Domain-Driven Design Learning"}},{"node":{"path":"/operations/end-to-end/","title":"12번가 전체 마이크로서비스의 배포"}},{"node":{"path":"/operations/apply-security-to-12st-mall/","title":"12번가 Mall에 토큰인증 적용하기"}},{"node":{"path":"/started/event-storming-learning/","title":"Event Storming Learning"}},{"node":{"path":"/operations/gitops-argo-cd/","title":"[GitOps] Argo CD 를 통한 카나리 배포"}},{"node":{"path":"/operations/azure/","title":"Azure Cloud Setup (AKS, ACR 설정)"}},{"node":{"path":"/started/","title":"Introduction"}},{"node":{"path":"/development/kafka-connect/","title":"CDC(Change Data Capture) with Kafka"}},{"node":{"path":"/custom-template/unit-test/","title":"Unit Test Creation Topping(New)"}},{"node":{"path":"/custom-template/mock-server/","title":"Open API 3.0-based Mock Server Generation Topping(New)"}},{"node":{"path":"/custom-template/tutorial/","title":"Concept of Custom Template"}},{"node":{"path":"/example-scenario/animal-hospital/","title":"Veterinary Practice Management System"}},{"node":{"path":"/example-scenario/food-delivery/","title":"food delivery"}},{"node":{"path":"/custom-template/designing-template/","title":"Developing Custom Template"}},{"node":{"path":"/example-scenario/accommodation-reservation/","title":"AirBnB"}},{"node":{"path":"/example-scenario/online-lecture/","title":"Internet lecture system"}},{"node":{"path":"/custom-template/custom-template/","title":"Custom Template Objects"}},{"node":{"path":"/example-scenario/library-system/","title":"library system"}},{"node":{"path":"/development/pubsub-idempotency/","title":"Pub/Sub Communication - Choreography with Idempotency"}},{"node":{"path":"/development/understanding-jpa-based-single-microservice/","title":"Understanding JPA Based Single Microservice"}},{"node":{"path":"/development/token-based-auth/","title":"JWT Token-based Authorization"}},{"node":{"path":"/development/pub-sub/","title":"Pub/Sub Communication"}},{"node":{"path":"/development/orchestration-saga/","title":"Orchestration Saga with Axon Framework"}},{"node":{"path":"/development/pubsub-deadline/","title":"Pub/Sub Communication - Choreography with Deadline added"}},{"node":{"path":"/development/oauth2with-keycloak/","title":"JWT Token-based Authorization - Advanced"}},{"node":{"path":"/development/monolith-2-misvc/","title":"MSA Communication by Req/Res"}},{"node":{"path":"/development/kafka-scaling/","title":"Kafka Scaling "}},{"node":{"path":"/development/kafka-scaling-concurrenty-handling/","title":"Kafka scaling & Concurrenty handling"}},{"node":{"path":"/development/kafka-retry-dlq/","title":"Kafka Retry & Dead Letter Queue "}},{"node":{"path":"/development/dp-frontend/","title":"Data Projection with Frontend and HATEOAS"}},{"node":{"path":"/development/kafka-basic/","title":"Kafka Basic Commands"}},{"node":{"path":"/development/gateway/","title":"API Gateway"}},{"node":{"path":"/development/conteact-messaging/","title":"Conteact Test by Message-based CDC"}},{"node":{"path":"/business/eventstorming-fooddelivery/","title":"Food Delivery Example"}},{"node":{"path":"/development/dp-graphql/","title":"Data Projection with GraphQL"}},{"node":{"path":"/development/contract-test/","title":"Contract Test (Consumer Driven Test)"}},{"node":{"path":"/development/dp-cqrs/","title":"Data Projection with CQRS"}},{"node":{"path":"/development/compensation-correlation/","title":"Pub/Sub Communication - Compensation & Correlation"}},{"node":{"path":"/development/choreography-saga/","title":"Choreography Saga with Axon Framework"}},{"node":{"path":"/development/cna-start/","title":"Running Microservice Units"}},{"node":{"path":"/development/circuit-breaker/","title":"Circuit Breaker for Req/Res"}},{"node":{"path":"/contact/question/","title":"CONTACT"}},{"node":{"path":"/business/ddd-google-drive/","title":"Google Drive Example"}},{"node":{"path":"/business/","title":"Shopping Mall Example"}}]}},"context":{}}