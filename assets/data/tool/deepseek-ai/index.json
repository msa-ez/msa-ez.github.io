{"hash":"cb34a4870b61bee8b2af18db22fb41baa2bd2932","data":{"markdownPage":{"id":"1350dc749ae56e0a7dcd0be3046a48dd","title":"","description":"","path":"/tool/deepseek-ai/","timeToRead":4,"content":"<!-- <h1>구내 설치형 AI기반 분석설계 및 구현, 배포<br>(MSAEZ + DeepSeek)</h1> -->\n<h1 id=\"guide-to-using-deepseek-model-in-msaez-runpod-cloud-gpu-environment\"><a href=\"#guide-to-using-deepseek-model-in-msaez-runpod-cloud-gpu-environment\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Guide to Using DeepSeek Model in MSAEZ (RunPod Cloud GPU Environment)</h1>\n<p><strong>MSAEZ</strong> now supports the use of <strong>DeepSeek AI</strong> inference models in a private cloud environment.</p>\n<p><strong>DeepSeek models</strong> are available in various parameter sizes including 7B and 67B, trained on over 2 trillion tokens of data. This data includes code, mathematical problems, and general text, making it applicable across various fields. Notably, most models are open-source under MIT or Apache 2.0 licenses.</p>\n<p>By utilizing the <strong>Ollama</strong> tool to install DeepSeek AI models directly in a local environment, you can reduce costs and dependencies on cloud-based AI services while freely using AI capabilities in an on-premises environment.</p>\n<p>Particularly, using DeepSeek AI enables requirement analysis and Domain-Driven Design (DDD) based cloud-native modeling through human-in-the-loop communication with designers. This allows for building more sophisticated microservice architectures while maintaining data consistency and flexible design.</p>\n<p>This guide explains <strong>how MSAEZ users can run DeepSeek AI models in a RunPod cloud GPU environment and integrate them with MSAEZ</strong>. It is intended for developers looking to build AI-based microservices using MSAEZ.\n<br><br></p>\n<!-- ## MSAEZ MSA 개발 프로세스\n\n<img src=\"https://github.com/user-attachments/assets/5fe9e0c5-064f-4969-ad9e-5389196f08f6\">\n<br>\nMSAEZ는 도메인 분석을 통해 마이크로서비스를 도출하고, 헥사고날 및 이벤트 드리븐 아키텍처 기반으로 설계한 후, 자동화된 테스트와 CI/CD 배포를 통해 안정적인 MSA 개발을 지원하는 전 주기에 걸친 프로세스를 제공합니다. \n<br><br>\n\n## 온프레미스 AI를 활용한 마이크로서비스 설계/구현/배포\n\n<img src=\"https://github.com/user-attachments/assets/b2851b91-543c-47a4-82d7-335ea0b1baa7\">\n<br>\nMSAEZ는 온프레미스 DeepSeek AI 모델을 활용하여 마이크로서비스를 자동 생성하고, 프로덕션 환경과 연동하여 마이크로서비스 설계를 자동화하고, AI를 활용한 자동화된 마이크로서비스 아키텍처 구축을 지원하여, 보다 효율적인 MSA 환경 도입을 가능하게 합니다. \n<br><br> -->\n<h2 id=\"cloud-gpu-service-configuration-for-deepseek-environment\"><a href=\"#cloud-gpu-service-configuration-for-deepseek-environment\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Cloud GPU Service Configuration for DeepSeek Environment</h2>\n<h3 id=\"setting-up-deepseek-model-environment-using-runpod\"><a href=\"#setting-up-deepseek-model-environment-using-runpod\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Setting up DeepSeek Model Environment Using RunPod</h3>\n<p><strong>1. You can create and request a new Pod through the Pods menu in <a href=\"https://runpod.io/\" target=\"_blank\" rel=\"noopener noreferrer\">RunPod</a>.</strong></p>\n<img style=\"margin-top: -20px;\" src=\"https://github.com/user-attachments/assets/8c1c8845-c031-4cb4-8cbb-596acc79fe47\">\n<ul>\n<li>The <code>deepseek-ai/DeepSeek-R1-Distill-Qwen-32B</code> model currently requires a VM with at least 80GB.</li>\n<li>While both community cloud and secure cloud options are available, we recommend using the <strong>community cloud</strong> due to current instability issues with secure cloud.</li>\n<li>We recommend the <code>4x RTX 4000 Ada</code> architecture; if unavailable, choose an instance with similar performance.\n<br><br></li>\n</ul>\n<p><strong>2. Click Edit Template to configure the template.</strong></p>\n<img src=\"https://github.com/user-attachments/assets/a39f6e9a-0651-4e58-96c7-74a45cf95c99\">\n<br>\n<img src=\"https://github.com/user-attachments/assets/c155ff28-3f51-47d0-96d7-e12952e6a8d9\">\n<ul>\n<li>\n<p>For template configuration, SGLang-based options like <code>Qwen 2.5 Coder 32B - SGLang by Relis</code> are stable.</p>\n<ul>\n<li><code>--tensor-parallel-size</code> activates tensor parallel processing and determines how many GPUs to distribute the model across. This helps overcome single GPU memory limitations and improves inference speed through parallel processing. Generally, the optimal value should <strong>match the number of available GPU instances</strong>. For example, when using four RTX 4000 Ada GPU instances, set --tensor-parallel-size to 4.</li>\n<li><code>--mem-fraction-static</code> is a parameter that sets <strong>what proportion of GPU memory to reserve statically before model execution</strong>. While GPU memory can be allocated dynamically, memory shortage errors are likely when long context sizes are passed. To prevent this, set to pre-occupy memory by the specified ratio. <strong>Generally, start with 0.8-0.9</strong> and <strong>adjust based on model size, context length, and GPU memory situation</strong>.</li>\n</ul>\n</li>\n</ul>\n<pre class=\"language-bash\"><code class=\"language-bash\">python3 <span class=\"token parameter variable\">-m</span> sglang.launch_server --model-path deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --context-length <span class=\"token number\">131072</span> <span class=\"token parameter variable\">--host</span> <span class=\"token number\">0.0</span>.0.0 <span class=\"token parameter variable\">--port</span> <span class=\"token number\">8000</span> --tensor-parallel-size <span class=\"token punctuation\">[</span>Number of GPU instances used<span class=\"token punctuation\">]</span> --api-key <span class=\"token punctuation\">[</span>API key <span class=\"token keyword\">for</span> LLM requests<span class=\"token punctuation\">]</span> --mem-fraction-static <span class=\"token number\">0.9</span> --disable-cuda-graph</code></pre>\n<img src=\"https://github.com/user-attachments/assets/92598e42-caa6-4977-9912-557914ee322f\">\n<br>\n<img src=\"https://github.com/user-attachments/assets/93c4499b-51a6-4ba5-9248-d7e3a9ccd1f0\">\n<ul>\n<li>For Volume Disk, you can allocate about 90GB as initial capacity considering model caching and various configuration files for the current Qwen 2.5 Coder 32B model.</li>\n<li>Set to On-Demand for stable operation without service interruption.\n<br><br></li>\n</ul>\n<h3 id=\"verifying-deepseek-model-configuration\"><a href=\"#verifying-deepseek-model-configuration\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Verifying DeepSeek Model Configuration</h3>\n<h4 id=\"checking-logs\"><a href=\"#checking-logs\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Checking Logs</h4>\n<img src=\"https://github.com/user-attachments/assets/73b97cce-9619-4739-80c5-039cf2d7ed23\">\n<br>\n<img src=\"https://github.com/user-attachments/assets/7d25d7fa-0b86-4159-b2ad-dbb23c1e2719\">\n<ul>\n<li>Check logs through <code>Log > Container</code>.</li>\n<li><code>The server is fired up and ready to roll</code>: This indicates when the system is actually ready for use.\n<br><br>\n</li>\n</ul>\n<h4 id=\"accessing\"><a href=\"#accessing\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Accessing</h4>\n<img src=\"https://github.com/user-attachments/assets/b26e1608-85b2-42df-9e08-0e6a9439a700\">\n<br>\n<ul>\n<li><code>Connect > HTTP Service</code></li>\n<li>The access URL is the path to the deployed Pod.</li>\n</ul>\n<pre class=\"language-bash\"><code class=\"language-bash\">https <span class=\"token parameter variable\">-v</span> POST <span class=\"token operator\">&lt;</span>Request Pod URL<span class=\"token operator\">></span>/v1/chat/completions <span class=\"token punctuation\">\\</span>\n  Authorization:<span class=\"token string\">\"Bearer &lt;API key for LLM requests>\"</span> <span class=\"token punctuation\">\\</span>\n  <span class=\"token assign-left variable\">model</span><span class=\"token operator\">=</span><span class=\"token string\">\"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\"</span> <span class=\"token punctuation\">\\</span>\n  messages:<span class=\"token operator\">=</span><span class=\"token string\">'[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]'</span></code></pre>\n<p><br><br></p>\n<h2 id=\"configuration-for-using-runpod-based-deepseek-model-in-msaez\"><a href=\"#configuration-for-using-runpod-based-deepseek-model-in-msaez\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Configuration for Using RunPod-based DeepSeek Model in MSAEZ</h2>\n<p>MSAEZ provides three model configurations to utilize DeepSeek models for various purposes: complexModel, standardModel, and simpleModel.</p>\n<ul>\n<li><code>complexModel</code>: Used for complex tasks requiring high performance, such as policy generation.</li>\n<li><code>standardModel</code>: Used for most general AI functions (e.g., text generation, Q&#x26;A). MSAEZ's core AI features are provided through <code>standardModel</code>.</li>\n<li><code>simpleModel</code>: Used for relatively simple tasks requiring quick processing, such as JSON object error correction.\n<br><br></li>\n</ul>\n<p><strong>1. Run the related Proxy server.</strong></p>\n<ul>\n<li>The <code>server.js</code> Proxy server mediates smooth communication between MSAEZ and RunPod, and MSAEZ efficiently provides various AI functions through the model configurations described above.</li>\n</ul>\n<pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token function\">node</span> ./server.js</code></pre>\n<p><strong>2. Modify localStorage values to use the related models.</strong></p>\n<pre class=\"language-js\"><code class=\"language-js\"><span class=\"token dom variable\">localStorage</span><span class=\"token punctuation\">.</span><span class=\"token property-access\">complexModel</span> <span class=\"token operator\">=</span> <span class=\"token string\">\"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\"</span>\n<span class=\"token dom variable\">localStorage</span><span class=\"token punctuation\">.</span><span class=\"token property-access\">standardModel</span> <span class=\"token operator\">=</span> <span class=\"token string\">\"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\"</span>\n<span class=\"token dom variable\">localStorage</span><span class=\"token punctuation\">.</span><span class=\"token property-access\">simpleModel</span> <span class=\"token operator\">=</span> <span class=\"token string\">\"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\"</span>\n<span class=\"token dom variable\">localStorage</span><span class=\"token punctuation\">.</span><span class=\"token property-access\">runpodUrl</span> <span class=\"token operator\">=</span> <span class=\"token string\">\"&lt;Request Pod URL>/v1/chat/completions\"</span></code></pre>\n<img src=\"https://github.com/user-attachments/assets/afa73078-4398-4187-979a-e789c75a574b\">\n<br>\n<p><strong>3. After testing, reset to empty values to return to default model usage.</strong></p>\n<pre class=\"language-js\"><code class=\"language-js\"><span class=\"token dom variable\">localStorage</span><span class=\"token punctuation\">.</span><span class=\"token property-access\">complexModel</span> <span class=\"token operator\">=</span> <span class=\"token string\">\"\"</span>\n<span class=\"token dom variable\">localStorage</span><span class=\"token punctuation\">.</span><span class=\"token property-access\">standardModel</span> <span class=\"token operator\">=</span> <span class=\"token string\">\"\"</span>\n<span class=\"token dom variable\">localStorage</span><span class=\"token punctuation\">.</span><span class=\"token property-access\">simpleModel</span> <span class=\"token operator\">=</span> <span class=\"token string\">\"\"</span></code></pre>\n<!-- <br><br>\n\n## On-Premises AI-based Analysis Design, Implementation, and Deployment (MSAEZ + DeepSeek)\n\n### Demo Application Using `DeepSeek-R1-Distill-Qwen-32B` Model - Civil Complaint Application Demo\n\n<div style=\"position: relative; padding-bottom: 56.25%; padding-top: 0px; height: 0; overflow: hidden;\">\n\t<iframe style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%;\" \n        src=\"https://www.youtube.com/embed/4PX4CWrdGCg?si=oD969pF_VGUpSf4Q&amp;start=3652\" \n        frameborder=\"0\" scrolling=\"no\" frameborder=\"none\" allowfullscreen=\"\">\n    </iframe>\n</div> -->\n","sidebar":"started","next":"","prev":"","headings":[{"depth":1,"value":"Guide to Using DeepSeek Model in MSAEZ (RunPod Cloud GPU Environment)","anchor":"#guide-to-using-deepseek-model-in-msaez-runpod-cloud-gpu-environment"},{"depth":2,"value":"Cloud GPU Service Configuration for DeepSeek Environment","anchor":"#cloud-gpu-service-configuration-for-deepseek-environment"},{"depth":3,"value":"Setting up DeepSeek Model Environment Using RunPod","anchor":"#setting-up-deepseek-model-environment-using-runpod"},{"depth":3,"value":"Verifying DeepSeek Model Configuration","anchor":"#verifying-deepseek-model-configuration"},{"depth":4,"value":"Checking Logs","anchor":"#checking-logs"},{"depth":4,"value":"Accessing","anchor":"#accessing"},{"depth":2,"value":"Configuration for Using RunPod-based DeepSeek Model in MSAEZ","anchor":"#configuration-for-using-runpod-based-deepseek-model-in-msaez"}]},"allMarkdownPage":{"edges":[{"node":{"path":"/tool/si-gpt/","title":"Code Implementation with ChatGPT"}},{"node":{"path":"/tool/setup-on-prem/","title":"Running on Docker Compose (with Github)"}},{"node":{"path":"/tool/on-prem-inst-gitea/","title":"On-Premises Installation Guide"}},{"node":{"path":"/tool/plsql-2-java/","title":"Legacy Modernizer"}},{"node":{"path":"/tool/pbc-marketplace/","title":"PBCs(Packaged Business Capabilities) for Composable Enterprise Implementation"}},{"node":{"path":"/tool/model-driven/","title":"Code Generation"}},{"node":{"path":"/tool/on-prem-inst/","title":"Installing on-premise MSA-Easy"}},{"node":{"path":"/tool/marketplace/","title":"Marketplace"}},{"node":{"path":"/tool/k8s-modeling/","title":"K8s Deployment Modeling"}},{"node":{"path":"/tool/infrastructure-modeling/","title":"Infrastructure Modeling (Kubernetes)"}},{"node":{"path":"/tool/google-drive-examples/","title":"Google Drive Examples"}},{"node":{"path":"/tool/event-storming-tool/","title":"EventStorming"}},{"node":{"path":"/tool/event-monitoring/","title":"Event Monitoring"}},{"node":{"path":"/tool/development-practice/","title":"Registration Course"}},{"node":{"path":"/tool/deepseek-ai/","title":""}},{"node":{"path":"/tool/ddl-to-eventstorming2/","title":"DDL To EventStorming"}},{"node":{"path":"/tool/ddl-to-eventstorming/","title":"DDL To EventStorming"}},{"node":{"path":"/tool/chat-gpt/","title":"Creating Models with ChatGPT"}},{"node":{"path":"/tool/cloud-ide-tool/","title":"Cloud IDE"}},{"node":{"path":"/tool/bc-domain-gen/","title":"Natural Language-based Bounded Context & Domain Design AI"}},{"node":{"path":"/tool/attending-lectures/","title":"Attending lectures"}},{"node":{"path":"/tool/aggregate-design/","title":"Aggregate Design"}},{"node":{"path":"/templates-language/springboot-java-template/","title":"Spring Boot/Java Template"}},{"node":{"path":"/templates-language/python-template/","title":"Python template "}},{"node":{"path":"/templates-language/go-template/","title":"Go Template "}},{"node":{"path":"/started/vibe-coding/","title":"Vibe Coding"}},{"node":{"path":"/started/key-features/","title":"Key Features"}},{"node":{"path":"/started/","title":"Introduction"}},{"node":{"path":"/started/event-storming-learning/","title":"Event Storming Learning"}},{"node":{"path":"/started/domain-driven/","title":"Domain-Driven Design Learning"}},{"node":{"path":"/operation/ops-deploy-diagramming-basic-objects/","title":"12st Mall Basic Deploy"}},{"node":{"path":"/operation/ops-deploy-diagramming-advanced-ingress/","title":"Ingress Deployment Model Design"}},{"node":{"path":"/operation/ops-deploy-diagramming-advanced-pvc/","title":"Persistent Volume"}},{"node":{"path":"/operation/ops-deploy-diagramming-advanced-istio/","title":"Istio Mesh"}},{"node":{"path":"/operation/ops-deploy-diagramming-advanced-hpa/","title":"Automatic Scaling (HPA) Deployment"}},{"node":{"path":"/info/pricing/","title":"Pricing"}},{"node":{"path":"/info/consulting/","title":"Consulting"}},{"node":{"path":"/info/partnership/","title":"Partner Program"}},{"node":{"path":"/info/company/","title":"Cases"}},{"node":{"path":"/example-scenario/online-lecture/","title":"Internet lecture system"}},{"node":{"path":"/example-scenario/library-system/","title":"library system"}},{"node":{"path":"/example-scenario/accommodation-reservation/","title":"AirBnB"}},{"node":{"path":"/example-scenario/food-delivery/","title":"food delivery"}},{"node":{"path":"/example-scenario/animal-hospital/","title":"Veterinary Practice Management System"}},{"node":{"path":"/development/pub-sub/","title":"Pub/Sub Integration"}},{"node":{"path":"/development/oauth2with-keycloak/","title":"JWT Token-based Authentication and Authorization"}},{"node":{"path":"/development/monolith-2-misvc/","title":"Request/Response Communication in MSA Integration"}},{"node":{"path":"/development/gateway/","title":"API Gateway"}},{"node":{"path":"/development/cna-start/","title":"Running Unit Microservices"}},{"node":{"path":"/development/dp-cqrs/","title":"Data Projection with CQRS"}},{"node":{"path":"/custom-template/unit-test/","title":"Test Automation"}},{"node":{"path":"/development/choreography-saga/","title":"Choreography Saga"}},{"node":{"path":"/custom-template/template-editor/","title":"Template Editor"}},{"node":{"path":"/custom-template/tutorial/","title":"Concept of Custom Template"}},{"node":{"path":"/custom-template/template-structure/","title":"Template File Structure"}},{"node":{"path":"/custom-template/template-editor-custom-template/","title":"Creating Custom Templates in MSAEZ"}},{"node":{"path":"/custom-template/mock-server/","title":"Open API 3.0-based Mock Server Generation Topping(New)"}},{"node":{"path":"/custom-template/loop-conditional-statement/","title":"Loop & Conditional Statement"}},{"node":{"path":"/custom-template/helper/","title":"Helper"}},{"node":{"path":"/custom-template/global-helper/","title":"Global Helper"}},{"node":{"path":"/custom-template/designing-template/","title":"Developing Custom Template"}},{"node":{"path":"/custom-template/custom-template/","title":"Custom Template Objects"}},{"node":{"path":"/business/","title":"Eventstorming - Shopping Mall"}},{"node":{"path":"/contact/question/","title":"CONTACT"}},{"node":{"path":"/business/eventstorming-fooddelivery/","title":"[이벤트스토밍] - DDD Food Delivery 예제"}},{"node":{"path":"/business/ddd-google-drive/","title":"[이벤트스토밍] - 구글 드라이브 예제"}}]}},"context":{}}